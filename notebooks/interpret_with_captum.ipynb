{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting BERT Models (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to interpret Bert models using  `Captum` library. In this particular case study we focus on a fine-tuned Question Answering model on SQUAD dataset using transformers library from Hugging Face: https://huggingface.co/transformers/\n",
    "\n",
    "We show how to use interpretation hooks to examine and better understand embeddings, sub-embeddings, bert, and attention layers. \n",
    "\n",
    "Note: Before running this tutorial, please install `seaborn`, `pandas` and `matplotlib`, `transformers`(from hugging face, tested on transformer version `4.3.0.dev0`) python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerIntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../../../ashankar/git/privacy-glue/runs/bert_base_uncased/policy_qa/seed_0/ were not used when initializing BertForSequenceClassification: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ../../../../ashankar/git/privacy-glue/runs/bert_base_uncased/policy_qa/seed_0/ and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model_path = '../../../../ashankar/git/privacy-glue/runs/bert_base_uncased/policy_qa/seed_0/'\n",
    "\n",
    "# load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to perform forward pass of the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a custom forward function that will allow us to access the start and end postitions of our prediction using `position` input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    preds = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    return np.argmax(preds, axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute attributions with respect to the `BertEmbeddings` layer.\n",
    "\n",
    "To do so, we need to define baselines / references, numericalize both the baselines and the inputs. We will define helper functions to achieve that.\n",
    "\n",
    "The cell below defines numericalized special tokens that will be later used for constructing inputs and corresponding baselines/references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a set of helper function for constructing references / baselines for word tokens, token types and position ids. We also provide separate helper functions that allow to construct attention masks and bert embeddings both for input and reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumalate couple samples in this array for visualization purposes\n",
    "vis_data_records_ig = []\n",
    "\n",
    "def interpret_sentence(model, sentence, min_len = 7, label = 0):\n",
    "    inputs = [tok.text for tok in tokenizer(sentence.lower())]\n",
    "\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # input_indices dim: [sequence_length]\n",
    "    seq_length = min_len\n",
    "\n",
    "    # predict\n",
    "    pred_ind = forward_func(inputs).item()\n",
    "\n",
    "    # generate reference indices for each sample\n",
    "    reference_indices = token_reference.generate_reference(seq_length, device=device).unsqueeze(0)\n",
    "\n",
    "    # compute attributions and approximation delta using layer integrated gradients\n",
    "    attributions_ig, delta = lig.attribute(input_indices, reference_indices, \\\n",
    "                                           n_steps=500, return_convergence_delta=True)\n",
    "\n",
    "    print('pred: ', Label.vocab.itos[pred_ind], '(', '%.2f'%pred, ')', ', delta: ', abs(delta))\n",
    "\n",
    "    add_attributions_to_visualizer(attributions_ig, text, pred, pred_ind, label, delta, vis_data_records_ig)\n",
    "    \n",
    "def add_attributions_to_visualizer(attributions, text, pred, pred_ind, label, delta, vis_data_records):\n",
    "    attributions = attributions.sum(dim=2).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    attributions = attributions.cpu().detach().numpy()\n",
    "\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    vis_data_records.append(visualization.VisualizationDataRecord(\n",
    "                            attributions,\n",
    "                            pred,\n",
    "                            Label.vocab.itos[pred_ind],\n",
    "                            Label.vocab.itos[label],\n",
    "                            Label.vocab.itos[1],\n",
    "                            attributions.sum(),\n",
    "                            text,\n",
    "                            delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the `question - text` pair that we'd like to use as an input for our Bert model and interpret what the model was forcusing on when predicting an answer to the question from given input text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = 'are my statistics kept private?', 'We will never share with or sell the information gained through the use of Apple HealthKit, such as age, weight and heart rate data, to advertisers or other agencies without your authorization.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's numericalize the question, the input text and generate corresponding baselines / references for all three sub-embeddings (word, token type and position embeddings) types using our helper functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's define the ground truth for prediction's start and end positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7882]\n"
     ]
    }
   ],
   "source": [
    "ground_truth = 'Relevant'\n",
    "\n",
    "ground_truth_tokens = tokenizer.encode(ground_truth, add_special_tokens=False)\n",
    "print(ground_truth_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions using input, token type, position id and a default attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'start_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [72], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m start_scores, end_scores \u001b[39m=\u001b[39m predict(input_ids, \\\n\u001b[1;32m      2\u001b[0m                                    token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids, \\\n\u001b[1;32m      3\u001b[0m                                    position_ids\u001b[39m=\u001b[39;49mposition_ids, \\\n\u001b[1;32m      4\u001b[0m                                    attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mQuestion: \u001b[39m\u001b[39m'\u001b[39m, question)\n\u001b[1;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mPredicted Answer: \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(all_tokens[torch\u001b[39m.\u001b[39margmax(start_scores) : torch\u001b[39m.\u001b[39margmax(end_scores)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]))\n",
      "Cell \u001b[0;32mIn [65], line 4\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(inputs, token_type_ids, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(inputs, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, position_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     output \u001b[39m=\u001b[39m model(inputs, token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[1;32m      3\u001b[0m                  position_ids\u001b[39m=\u001b[39mposition_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, )\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39;49mstart_logits, output\u001b[39m.\u001b[39mend_logits\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'start_logits'"
     ]
    }
   ],
   "source": [
    "start_scores, end_scores = predict(input_ids, \\\n",
    "                                   token_type_ids=token_type_ids, \\\n",
    "                                   position_ids=position_ids, \\\n",
    "                                   attention_mask=attention_mask)\n",
    "\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two different ways of computing the attributions for emebdding layers. One option is to use `LayerIntegratedGradients` and compute the attributions with respect to `BertEmbedding`. The second option is to use `LayerIntegratedGradients` for each `word_embeddings`, `token_type_embeddings` and `position_embeddings` and compute the attributions w.r.t each embedding vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SequenceClassifierOutput' object has no attribute 'start_logits'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [73], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m lig \u001b[39m=\u001b[39m LayerIntegratedGradients(squad_pos_forward_func, model\u001b[39m.\u001b[39mbert\u001b[39m.\u001b[39membeddings)\n\u001b[0;32m----> 3\u001b[0m attributions_start, delta_start \u001b[39m=\u001b[39m lig\u001b[39m.\u001b[39;49mattribute(inputs\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m      4\u001b[0m                                   baselines\u001b[39m=\u001b[39;49mref_input_ids,\n\u001b[1;32m      5\u001b[0m                                   additional_forward_args\u001b[39m=\u001b[39;49m(token_type_ids, position_ids, attention_mask, \u001b[39m0\u001b[39;49m),\n\u001b[1;32m      6\u001b[0m                                   return_convergence_delta\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      7\u001b[0m attributions_end, delta_end \u001b[39m=\u001b[39m lig\u001b[39m.\u001b[39mattribute(inputs\u001b[39m=\u001b[39minput_ids, baselines\u001b[39m=\u001b[39mref_input_ids,\n\u001b[1;32m      8\u001b[0m                                 additional_forward_args\u001b[39m=\u001b[39m(token_type_ids, position_ids, attention_mask, \u001b[39m1\u001b[39m),\n\u001b[1;32m      9\u001b[0m                                 return_convergence_delta\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/captum/log/__init__.py:35\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m---> 35\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/captum/attr/_core/layer/layer_integrated_gradients.py:365\u001b[0m, in \u001b[0;36mLayerIntegratedGradients.attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice_ids \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_func, \u001b[39m\"\u001b[39m\u001b[39mdevice_ids\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m--> 365\u001b[0m inputs_layer \u001b[39m=\u001b[39m _forward_layer_eval(\n\u001b[1;32m    366\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_func,\n\u001b[1;32m    367\u001b[0m     inps,\n\u001b[1;32m    368\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer,\n\u001b[1;32m    369\u001b[0m     device_ids\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice_ids,\n\u001b[1;32m    370\u001b[0m     additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[1;32m    371\u001b[0m     attribute_to_layer_input\u001b[39m=\u001b[39;49mattribute_to_layer_input,\n\u001b[1;32m    372\u001b[0m )\n\u001b[1;32m    374\u001b[0m \u001b[39m# if we have one output\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer, \u001b[39mlist\u001b[39m):\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/captum/_utils/gradient.py:182\u001b[0m, in \u001b[0;36m_forward_layer_eval\u001b[0;34m(forward_fn, inputs, layer, additional_forward_args, device_ids, attribute_to_layer_input, grad_enabled)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward_layer_eval\u001b[39m(\n\u001b[1;32m    174\u001b[0m     forward_fn: Callable,\n\u001b[1;32m    175\u001b[0m     inputs: Union[Tensor, Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    180\u001b[0m     grad_enabled: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    181\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m], List[Tuple[Tensor, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[0;32m--> 182\u001b[0m     \u001b[39mreturn\u001b[39;00m _forward_layer_eval_with_neuron_grads(\n\u001b[1;32m    183\u001b[0m         forward_fn,\n\u001b[1;32m    184\u001b[0m         inputs,\n\u001b[1;32m    185\u001b[0m         layer,\n\u001b[1;32m    186\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[1;32m    187\u001b[0m         gradient_neuron_selector\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    188\u001b[0m         grad_enabled\u001b[39m=\u001b[39;49mgrad_enabled,\n\u001b[1;32m    189\u001b[0m         device_ids\u001b[39m=\u001b[39;49mdevice_ids,\n\u001b[1;32m    190\u001b[0m         attribute_to_layer_input\u001b[39m=\u001b[39;49mattribute_to_layer_input,\n\u001b[1;32m    191\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/captum/_utils/gradient.py:445\u001b[0m, in \u001b[0;36m_forward_layer_eval_with_neuron_grads\u001b[0;34m(forward_fn, inputs, layer, additional_forward_args, gradient_neuron_selector, grad_enabled, device_ids, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    442\u001b[0m grad_enabled \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m \u001b[39mif\u001b[39;00m gradient_neuron_selector \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m grad_enabled\n\u001b[1;32m    444\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_grad_enabled(grad_enabled):\n\u001b[0;32m--> 445\u001b[0m     saved_layer \u001b[39m=\u001b[39m _forward_layer_distributed_eval(\n\u001b[1;32m    446\u001b[0m         forward_fn,\n\u001b[1;32m    447\u001b[0m         inputs,\n\u001b[1;32m    448\u001b[0m         layer,\n\u001b[1;32m    449\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[1;32m    450\u001b[0m         attribute_to_layer_input\u001b[39m=\u001b[39;49mattribute_to_layer_input,\n\u001b[1;32m    451\u001b[0m     )\n\u001b[1;32m    452\u001b[0m device_ids \u001b[39m=\u001b[39m _extract_device_ids(forward_fn, saved_layer, device_ids)\n\u001b[1;32m    453\u001b[0m \u001b[39m# Identifies correct device ordering based on device ids.\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[39m# key_list is a list of devices in appropriate ordering for concatenation.\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[39m# If only one key exists (standard model), key list simply has one element.\u001b[39;00m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/captum/_utils/gradient.py:294\u001b[0m, in \u001b[0;36m_forward_layer_distributed_eval\u001b[0;34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m             all_hooks\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    292\u001b[0m                 single_layer\u001b[39m.\u001b[39mregister_forward_hook(hook_wrapper(single_layer))\n\u001b[1;32m    293\u001b[0m             )\n\u001b[0;32m--> 294\u001b[0m     output \u001b[39m=\u001b[39m _run_forward(\n\u001b[1;32m    295\u001b[0m         forward_fn,\n\u001b[1;32m    296\u001b[0m         inputs,\n\u001b[1;32m    297\u001b[0m         target\u001b[39m=\u001b[39;49mtarget_ind,\n\u001b[1;32m    298\u001b[0m         additional_forward_args\u001b[39m=\u001b[39;49madditional_forward_args,\n\u001b[1;32m    299\u001b[0m     )\n\u001b[1;32m    300\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    301\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m all_hooks:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/captum/_utils/common.py:456\u001b[0m, in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    453\u001b[0m inputs \u001b[39m=\u001b[39m _format_input(inputs)\n\u001b[1;32m    454\u001b[0m additional_forward_args \u001b[39m=\u001b[39m _format_additional_forward_args(additional_forward_args)\n\u001b[0;32m--> 456\u001b[0m output \u001b[39m=\u001b[39m forward_func(\n\u001b[1;32m    457\u001b[0m     \u001b[39m*\u001b[39;49m(\u001b[39m*\u001b[39;49minputs, \u001b[39m*\u001b[39;49madditional_forward_args)\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;49;00m additional_forward_args \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    459\u001b[0m     \u001b[39melse\u001b[39;49;00m inputs\n\u001b[1;32m    460\u001b[0m )\n\u001b[1;32m    461\u001b[0m \u001b[39mreturn\u001b[39;00m _select_targets(output, target)\n",
      "Cell \u001b[0;32mIn [66], line 2\u001b[0m, in \u001b[0;36msquad_pos_forward_func\u001b[0;34m(inputs, token_type_ids, position_ids, attention_mask, position)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msquad_pos_forward_func\u001b[39m(inputs, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, position_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, position\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     pred \u001b[39m=\u001b[39m predict(inputs,\n\u001b[1;32m      3\u001b[0m                    token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[1;32m      4\u001b[0m                    position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m      5\u001b[0m                    attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m      6\u001b[0m     pred \u001b[39m=\u001b[39m pred[position]\n\u001b[1;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m pred\u001b[39m.\u001b[39mmax(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mvalues\n",
      "Cell \u001b[0;32mIn [65], line 4\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(inputs, token_type_ids, position_ids, attention_mask)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(inputs, token_type_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, position_ids\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, attention_mask\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      2\u001b[0m     output \u001b[39m=\u001b[39m model(inputs, token_type_ids\u001b[39m=\u001b[39mtoken_type_ids,\n\u001b[1;32m      3\u001b[0m                  position_ids\u001b[39m=\u001b[39mposition_ids, attention_mask\u001b[39m=\u001b[39mattention_mask, )\n\u001b[0;32m----> 4\u001b[0m     \u001b[39mreturn\u001b[39;00m output\u001b[39m.\u001b[39;49mstart_logits, output\u001b[39m.\u001b[39mend_logits\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SequenceClassifierOutput' object has no attribute 'start_logits'"
     ]
    }
   ],
   "source": [
    "lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)\n",
    "\n",
    "attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "                                  baselines=ref_input_ids,\n",
    "                                  additional_forward_args=(token_type_ids, position_ids, attention_mask, 0),\n",
    "                                  return_convergence_delta=True)\n",
    "attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "                                additional_forward_args=(token_type_ids, position_ids, attention_mask, 1),\n",
    "                                return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to summarize attributions for each word token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_start_sum = summarize_attributions(attributions_start)\n",
    "attributions_end_sum = summarize_attributions(attributions_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Visualizations For Start Position \u001b[0m\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [76], line 23\u001b[0m\n\u001b[1;32m     12\u001b[0m end_position_vis \u001b[39m=\u001b[39m viz\u001b[39m.\u001b[39mVisualizationDataRecord(\n\u001b[1;32m     13\u001b[0m                         attributions_end_sum,\n\u001b[1;32m     14\u001b[0m                         torch\u001b[39m.\u001b[39mmax(torch\u001b[39m.\u001b[39msoftmax(end_scores[\u001b[39m0\u001b[39m], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m                         all_tokens,\n\u001b[1;32m     20\u001b[0m                         delta_end)\n\u001b[1;32m     22\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[1m\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVisualizations For Start Position\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m viz\u001b[39m.\u001b[39;49mvisualize_text([start_position_vis])\n\u001b[1;32m     25\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[1m\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mVisualizations For End Position\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m\\033\u001b[39;00m\u001b[39m[0m\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m viz\u001b[39m.\u001b[39mvisualize_text([end_position_vis])\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/captum/attr/_utils/visualization.py:543\u001b[0m, in \u001b[0;36mvisualize_text\u001b[0;34m(datarecords, legend)\u001b[0m\n\u001b[1;32m    523\u001b[0m rows \u001b[39m=\u001b[39m [\n\u001b[1;32m    524\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m<tr><th>True Label</th>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    525\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m<th>Predicted Label</th>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m<th>Word Importance</th>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    529\u001b[0m ]\n\u001b[1;32m    530\u001b[0m \u001b[39mfor\u001b[39;00m datarecord \u001b[39min\u001b[39;00m datarecords:\n\u001b[1;32m    531\u001b[0m     rows\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    532\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\n\u001b[1;32m    533\u001b[0m             [\n\u001b[1;32m    534\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m<tr>\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    535\u001b[0m                 format_classname(datarecord\u001b[39m.\u001b[39mtrue_class),\n\u001b[1;32m    536\u001b[0m                 format_classname(\n\u001b[1;32m    537\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{1:.2f}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    538\u001b[0m                         datarecord\u001b[39m.\u001b[39mpred_class, datarecord\u001b[39m.\u001b[39mpred_prob\n\u001b[1;32m    539\u001b[0m                     )\n\u001b[1;32m    540\u001b[0m                 ),\n\u001b[1;32m    541\u001b[0m                 format_classname(datarecord\u001b[39m.\u001b[39mattr_class),\n\u001b[1;32m    542\u001b[0m                 format_classname(\u001b[39m\"\u001b[39m\u001b[39m{0:.2f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(datarecord\u001b[39m.\u001b[39mattr_score)),\n\u001b[0;32m--> 543\u001b[0m                 format_word_importances(\n\u001b[1;32m    544\u001b[0m                     datarecord\u001b[39m.\u001b[39;49mraw_input_ids, datarecord\u001b[39m.\u001b[39;49mword_attributions\n\u001b[1;32m    545\u001b[0m                 ),\n\u001b[1;32m    546\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m<tr>\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    547\u001b[0m             ]\n\u001b[1;32m    548\u001b[0m         )\n\u001b[1;32m    549\u001b[0m     )\n\u001b[1;32m    551\u001b[0m \u001b[39mif\u001b[39;00m legend:\n\u001b[1;32m    552\u001b[0m     dom\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    553\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m<div style=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mborder-top: 1px solid; margin-top: 5px; \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[39m        padding-top: 5px; display: inline-block\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m>\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    555\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/captum/attr/_utils/visualization.py:500\u001b[0m, in \u001b[0;36mformat_word_importances\u001b[0;34m(words, importances)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mif\u001b[39;00m importances \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m \u001b[39mlen\u001b[39m(importances) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    499\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m<td></td>\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 500\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(words) \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(importances)\n\u001b[1;32m    501\u001b[0m tags \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m<td>\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    502\u001b[0m \u001b[39mfor\u001b[39;00m word, importance \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(words, importances[: \u001b[39mlen\u001b[39m(words)]):\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# storing couple samples in an array for visualization purposes\n",
    "start_position_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_start_sum,\n",
    "                        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                        torch.argmax(start_scores),\n",
    "                        torch.argmax(start_scores),\n",
    "                        str(ground_truth_start_ind),\n",
    "                        attributions_start_sum.sum(),       \n",
    "                        all_tokens,\n",
    "                        delta_start)\n",
    "\n",
    "end_position_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_end_sum,\n",
    "                        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "                        torch.argmax(end_scores),\n",
    "                        torch.argmax(end_scores),\n",
    "                        str(ground_truth_end_ind),\n",
    "                        attributions_end_sum.sum(),       \n",
    "                        all_tokens,\n",
    "                        delta_end)\n",
    "\n",
    "print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "viz.visualize_text([start_position_vis])\n",
    "\n",
    "print('\\033[1m', 'Visualizations For End Position', '\\033[0m')\n",
    "viz.visualize_text([end_position_vis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'img/bert/visuals_of_start_end_predictions.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[0;32m----> 2\u001b[0m Image(filename\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mimg/bert/visuals_of_start_end_predictions.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/IPython/core/display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[0;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[1;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconfined \u001b[39m=\u001b[39m unconfined\n\u001b[1;32m    969\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malt \u001b[39m=\u001b[39m alt\n\u001b[0;32m--> 970\u001b[0m \u001b[39msuper\u001b[39;49m(Image, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, url\u001b[39m=\u001b[39;49murl, filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[1;32m    971\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata)\n\u001b[1;32m    973\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m, {}):\n\u001b[1;32m    974\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m metadata[\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/IPython/core/display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[0;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m {}\n\u001b[0;32m--> 327\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m    328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_data()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/IPython/core/display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[39m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[1;32m   1004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[0;32m-> 1005\u001b[0m     \u001b[39msuper\u001b[39;49m(Image,\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mreload()\n\u001b[1;32m   1006\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretina:\n\u001b[1;32m   1007\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retina_shape()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/privacy-glue-BX6ttsfR-py3.8/lib/python3.8/site-packages/IPython/core/display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    352\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_flags \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 353\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_flags, encoding\u001b[39m=\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    354\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m    355\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    356\u001b[0m     \u001b[39m# Deferred import\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'img/bert/visuals_of_start_end_predictions.png'"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='img/bert/visuals_of_start_end_predictions.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above we can tell that for predicting start position our model is focusing more on the question side. More specifically on the tokens `what` and `important`. It has also slight focus on the token sequence `to us` in the text side.\n",
    "\n",
    "In contrast to that, for predicting end position, our model focuses more on the text side and has relative high attribution on the last end position token `kinds`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('privacy-glue-BX6ttsfR-py3.8')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "c471bd70f1f20ae74bfe3c3ffe3c0f924bf6266d978205bc9d9da83b3c26654d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
